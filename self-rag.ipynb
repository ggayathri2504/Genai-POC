{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai,os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "openai.api_key = \"\"\n",
    "embeding_func = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma(persist_directory=\"chroma_db\",embedding_function=embeding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from typing import Annotated,Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    status_count = state_dict['status_count'] + 1\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    print(documents)\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question,\"status_count\":status_count}}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Run\n",
    "    with get_openai_callback() as cb:\n",
    "        generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "        print(f'{cb} prompt tokens counted by the Openai API')\n",
    "        \n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation,\"status_count\":status_count}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    \n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "        \n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        with get_openai_callback() as cb:\n",
    "            score = chain.invoke({\"question\": question, \"context\": d.page_content})\n",
    "            print(f'{cb} prompt tokens counted by the Openai API')\n",
    "        grade = score[0].binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"keys\": {\"documents\": filtered_docs, \"question\": question,\"status_count\":status_count}}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question: \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    with get_openai_callback() as cb:\n",
    "        better_question = chain.invoke({\"question\": question})\n",
    "        print(f'{cb} prompt tokens counted by the Openai API')\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question,\"status_count\":status_count}}\n",
    "\n",
    "\n",
    "def prepare_for_final_grade(state):\n",
    "    \"\"\"\n",
    "    Passthrough state for final grade.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The current graph state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---FINAL GRADE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation,\"status_count\":status_count}\n",
    "    }\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    status_count = state_dict['status_count']\n",
    "    if status_count >=3:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        if not filtered_documents:\n",
    "            # All documents have been filtered check_relevance\n",
    "            # We will re-generate a new query\n",
    "            print(\"---DECISION: TRANSFORM QUERY---\")\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            # We have relevant documents, so generate answer\n",
    "            print(\"---DECISION: GENERATE---\")\n",
    "            return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION vs DOCUMENTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Supported score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {documents} \n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is grounded in / supported by a set of facts.\"\"\",\n",
    "        input_variables=[\"generation\", \"documents\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    with get_openai_callback() as cb:\n",
    "        score = chain.invoke({\"generation\": generation, \"documents\": documents})\n",
    "        print(f'{cb} prompt tokens counted by the Openai API')\n",
    "    grade = score[0].binary_score\n",
    "    \n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\")\n",
    "        return \"supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT SUPPORTED, GENERATE AGAIN---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "def grade_generation_v_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "    status_count = state_dict['status_count']\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Useful score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "        Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation} \n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\"\"\",\n",
    "        input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "    with get_openai_callback() as cb:\n",
    "        score = chain.invoke({\"generation\": generation, \"question\": question})\n",
    "        print(f'{cb} prompt tokens counted by the Openai API')\n",
    "    grade = score[0].binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: USEFUL---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        # if should_continue(state) == \"END\":\n",
    "            # print(\"---DECISION: USEFUL---\")\n",
    "            # return \"END\"\n",
    "        print(\"---DECISION: NOT USEFUL---\")\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)  # passthrough\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "        \"END\": END\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents,\n",
    "    {\n",
    "        \"supported\": \"prepare_for_final_grade\",\n",
    "        \"not supported\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    grade_generation_v_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "[Document(page_content='Univar\\nSolutions\\nSafety Data Sheet\\nD-LIMONENE\\nVersion 1.7\\nRevision Date: 11/11/2023\\nSECTION 7. HANDLING AND STORAGE\\nAdvice on protection against\\n:\\nDo not spray on a naked flame or any incandescent material.\\nfire and explosion\\nTake necessary action to avoid static electricity discharge\\n(which might cause ignition of organic vapours). Keep away\\nfrom open flames, hot surfaces and sources of ignition.\\nAdvice on safe handling\\n:\\nAvoid formation of aerosol.\\nDo not breathe vapours/dust.\\nAvoid exposure - obtain special instructions before use.\\nAvoid contact with skin and eyes.\\nFor personal protection see section 8.\\nSmoking, eating and drinking should be prohibited in the ap-\\nplication area.\\nTake precautionary measures against static discharges.\\nProvide sufficient air exchange and/or exhaust in work rooms.\\nOpen drum carefully as content may be under pressure.\\nDispose of rinse water in accordance with local and national\\nregulations.\\nPersons susceptible to skin sensitisation problems or asthma,\\nallergies, chronic or recurrent respiratory disease should not\\nbe employed in any process in which this mixture is being\\nused.\\nConditions for safe storage\\n:\\nNo smoking.\\nKeep container tightly closed in a dry and well-ventilated\\nplace.\\nContainers which are opened must be carefully resealed and\\nkept upright to prevent leakage.\\nObserve label precautions.\\nElectrical installations / working materials must comply with\\nthe technological safety standards.\\nSECTION 8. EXPOSURE CONTROLS/PERSONAL PROTECTION\\nComponents with workplace control parameters\\nContains no substances with occupational exposure limit values.\\nPersonal protective equipment\\nRespiratory protection\\n:\\nNo personal respiratory protective equipment normally re-\\nquired.\\nIn the case of vapour formation use a respirator with an ap-\\nproved filter.\\nHand protection\\nRemarks\\n:\\nThe suitability for a specific workplace should be discussed\\nwith the producers of the protective gloves.\\nEye protection\\n:\\nEye wash bottle with pure water\\nTightly fitting safety goggles\\nSkin and body protection\\n:\\nImpervious clothing\\nChoose body protection according to the amount and concen-\\ntration of the dangerous substance at the work place.\\nHygiene measures\\n:\\nWhen using do not eat or drink.\\nWhen using do not smoke.\\nSDS Number: 100000003281\\n4 / 12\\nD-LIMONENE', metadata={'file_path': 'D-Limonene SDS.pdf', 'page_number': 4, 'source': 'Uploaded', 'total_pages': 13}), Document(page_content='Univar\\nSolutions\\nSafety Data Sheet\\nD-LIMONENE\\nVersion 1.7\\nRevision Date: 11/11/2023\\nSubstances\\nRemarks: This product neither contains, nor was manufac-\\ntured with a Class I or Class II ODS as defined by the U.S.\\nClean Air Act Section 602 (40 CFR 82, Subpt. A, App.A + B).\\nAdditional ecological infor-\\n:\\nAn environmental hazard cannot be excluded in the event of\\nmation\\nunprofessional handling or disposal.\\nVery toxic to aquatic life with long lasting effects.\\nSECTION 13. DISPOSAL CONSIDERATIONS\\nDisposal methods\\nWaste from residues\\n:\\nDispose of in accordance with all applicable local, state and\\nfederal regulations.\\nFor assistance with your waste management needs - including\\ndisposal, recycling and waste stream reduction, contact Uni-\\nvar Solutions ChemCare: 1-800-637-7922\\nContaminated packaging\\n:\\nEmpty remaining contents.\\nDispose of as unused product.\\nDo not re-use empty containers.\\nDo not burn, or use a cutting torch on, the empty drum.\\nSECTION 14. TRANSPORT INFORMATION\\nDOT (Department of Transportation):\\nUN2319, TERPENE HYDROCARBONS, N.O.S., 3, III\\nIATA (International Air Transport Association):\\nUN2319, TERPENE HYDROCARBONS, N.O.S., 3, III\\nIMDG (International Maritime Dangerous Goods):\\nUN2319, TERPENE HYDROCARBONS, N.O.S., 3, III, Marine Pollutant (D-LIMONENE) , Flash Point:46 °C(115\\n°F)\\nSpecial Notes:\\n:\\nThe flash point for this material is greater than 100 F (38 C).\\nTherefore, in accordance with 49 CFR 173.150(f) non-bulk\\ncontainers (<450L or <119 gallon capacity) of this material\\nmay be shipped as non-regulated when transported solely by\\nland, as long as the material is not a hazardous waste, a ma-\\nrine pollutant, or specifically listed as a hazardous substance.\\nSECTION 15. REGULATORY INFORMATION\\nWHMIS Classification\\n:\\nB3: Combustible Liquid\\nD2B: Toxic Material Causing Other Toxic Effects\\nEPCRA - Emergency Planning and Community Right-to-Know Act\\nCERCLA Reportable Quantity\\nThis material does not contain any components with a CERCLA RQ.\\nSDS Number: 100000003281\\n9 / 12\\nD-LIMONENE', metadata={'file_path': 'D-Limonene SDS.pdf', 'page_number': 9, 'source': 'Uploaded', 'total_pages': 13}), Document(page_content='Univar\\nSolutions\\nSafety Data Sheet\\nD-LIMONENE\\nVersion 1.7\\nRevision Date: 11/11/2023\\nKECI\\n:\\nOn the inventory, or in compliance with the inventory\\nPHIL\\n:\\nOn the inventory, or in compliance with the inventory\\nIECSC\\n:\\nOn the inventory, or in compliance with the inventory\\nSECTION16. OTHER INFORMATION\\nNFPA:\\nHMIS III:\\nFlammability\\nHEALTH\\n3\\n2\\nInstability\\n2\\n0\\nFLAMMABILITY\\n3\\nHealth\\nPHYSICAL HAZARD\\n0\\n0 = not significant, 1 =Slight,\\nSpecial hazard\\n2 = Moderate, 3 = High\\n4 =Extreme, * = Chronic\\nThe information accumulated is based on the data of which we are aware and is believed\\nto be correct as of the date hereof. Since this information may be applied under conditions\\nbeyond our control and with which we may be unfamiliar and since data made become\\navailable subsequently to the date hereof, we do not assume any responsibility for the\\nresults of its use. Recipients are advised to confirm in advance of need that the information\\nis current, applicable, and suitable to their circumstances. This SDS has been prepared by\\nUnivar Solutions Product Compliance Department (1-855-429-2661)\\nSDSNA@univarsolutions.com.\\nRevision Date\\n:\\n11/11/2023\\nLegacy SDS:\\n:\\nR0111643\\nMaterial number:\\n16177929, 16153034, 16145536, 16153266, 16151009, 16133660, 16122015, 16120696,\\n16062075, 16056042, 16056014, 16056013, 16056012, 16056011, 16037740, 16013558,\\n763681, 728284, 554323, 554287, 554254, 554210, 554139, 508584, 504369, 139240,\\n104220, 103451, 103445, 103335, 102799, 87227, 87063, 86953, 86747, 70685, 70604,\\n70040, 69374, 55064, 54193, 20823, 20822\\nKey or legend to abbreviations and acronyms used in the safety data sheet\\nACGIH\\nAmerican Conference of Govern-\\nLD50\\nLethal Dose 50%\\nment Industrial Hygienists\\nAICS\\nAustralia, Inventory of Chemical\\nLOAEL\\nLowest Observed Adverse Effect\\nSubstances\\nLevel\\nDSL\\nCanada, Domestic Substances List\\nNFPA\\nNational Fire Protection Agency\\nSDS Number: 100000003281\\n11 / 12\\nD-LIMONENE', metadata={'file_path': 'D-Limonene SDS.pdf', 'page_number': 11, 'source': 'Uploaded', 'total_pages': 13}), Document(page_content=\"Univar\\nSolutions\\nSafety Data Sheet\\nD-LIMONENE\\nVersion 1.7\\nRevision Date: 11/11/2023\\nSECTION 1. PRODUCT AND COMPANY IDENTIFICATION\\nProduct name\\n:\\nD-LIMONENE\\nManufacturer or supplier's details\\nCompany\\n:\\nUnivar Solutions USA\\nAddress\\n3075 Highland Pkwy Suite 200\\nDowners Grove, IL 60515\\nUnited States of America (USA)\\nEmergency telephone number:\\nTransport North America: CHEMTREC (1-800-424-9300)\\nCHEMTREC INTERNATIONAL Tel # 703-527-3887\\nAdditional Information:\\n:\\nResponsible Party: Product Compliance Department\\nE-mail: SDSNA@univarsolutions.com\\nSDS Requests: 1-855-429-2661\\nWebsite: www.univarsolutions.com\\nSECTION 2. HAZARDS IDENTIFICATION\\nGHS Classification\\nFlammable liquids\\n:\\nCategory 3\\nSkin irritation\\n:\\nCategory 2\\nEye irritation\\n:\\nCategory 2B\\nSkin sensitisation\\n:\\nCategory 1\\nAspiration hazard\\n:\\nCategory 1\\nGHS label elements\\nHazard pictograms\\n:\\nSignal word\\n:\\nDanger\\nHazard statements\\n:\\nH226 Flammable liquid and vapour.\\nH304 May be fatal if swallowed and enters airways.\\nH315 + H320 Causes skin and eye irritation.\\nH317 May cause an allergic skin reaction.\\nPrecautionary statements\\n:\\nPrevention:\\nP210 Keep away from heat/ sparks/ open flames/ hot surfaces.\\nNo smoking.\\nP233 Keep container tightly closed.\\nP240 Ground/bond container and receiving equipment.\\nP241 Use explosion-proof electrical/ ventilating/ lighting equip-\\nment.\\nP242 Use only non-sparking tools.\\nP243 Take precautionary measures against static discharge.\\nP261 Avoid breathing dust/ fume/ gas/ mist/ vapours/ spray.\\nSDS Number: 100000003281\\n1 / 12\\nD-LIMONENE\", metadata={'file_path': 'D-Limonene SDS.pdf', 'page_number': 1, 'source': 'Uploaded', 'total_pages': 13})]\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "Tokens Used: 1247\n",
      "\tPrompt Tokens: 1155\n",
      "\tCompletion Tokens: 92\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0019165 prompt tokens counted by the Openai API\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---GRADE GENERATION vs DOCUMENTS---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\n",
      "---FINAL GRADE---\n",
      "\"Node 'prepare_for_final_grade':\"\n",
      "'\\n---\\n'\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0 prompt tokens counted by the Openai API\n",
      "---DECISION: USEFUL---\n",
      "\"Node '__end__':\"\n",
      "'\\n---\\n'\n",
      "('Special notes for shipping include that the flash point for the material is '\n",
      " 'greater than 100 F (38 C), allowing non-bulk containers to be shipped as '\n",
      " 'non-regulated when transported solely by land. It is important to ensure '\n",
      " 'compliance with regulations and not transport the material as a hazardous '\n",
      " 'waste or marine pollutant. For further assistance with waste management '\n",
      " 'needs, including disposal and recycling, contact Univar Solutions ChemCare '\n",
      " 'at 1-800-637-7922.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"keys\": {\"question\": \"Any special notes for shipping?\",\"status_count\":0}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "try:\n",
    "    pprint.pprint(value[\"keys\"][\"generation\"])\n",
    "except Exception as e:\n",
    "    pprint.pprint(\"i couldn't find the answer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
